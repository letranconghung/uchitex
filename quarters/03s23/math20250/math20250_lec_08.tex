\lecture{8}{20 Apr 2023}{Determinant}

\begin{motivation}
    The motivation for representing matrices in such a manner now becomes clearer for us. Let \(\psi_1: \bbk^l \to \bbk^n, \psi_2: \bbk^n \to \bbk^m\) be linear transformations with corresponding \(T_1 \in \bbm_{n \times l}(\bbk), T_2 \in \bbm_{m \times n}(\bbk)\):

    \[
        \bbk^l \xrightarrow{\psi_1, T_1} \bbk^n \xrightarrow{\psi_2, T_2} \bbk^m
    \]

    then it is also an exercise to show that \(\psi_2 \circ \psi_1\) is also a linear transformation, that corresponds to \(T_2 \cdot T_1 \in \bbm_{m \times l}(\bbk)\).

    Matrix multiplication is therefore built in such a way that \(T_2 \cdot T_1\) results in an \(m \times l\) matrix. It makes sense to multiply in such a way to fit the shape requirements: \(i-\)th row by \(j-\)th column.
\end{motivation}

\begin{recall}
    \(D: \bbm_n(\bbk) \to \bbk\) is a function that is multilinear, alternating and satisfies: \(D(I_n) = 1\). As of now, we don't know if this function exists at all!
\end{recall}

\begin{remark}
    Assuming that \(D\) is multilinear, then the first condition for alternating implies the second. When \(2 \neq 0\), the second condition imlpies the first one.
\end{remark}

\begin{proof} {Remark}
    \pffwd We want to show that \[
        D\begin{pmatrix}
                r_1 \\ \vdots \\ r_n
            \end{pmatrix} = 0 \:\text{whenever}\: \exists i \neq j: r_i = r_j \implies D \begin{pmatrix}
                \vdots \\ r_i \\ r_{i+1} \\ \vdots
            \end{pmatrix} = -D \begin{pmatrix}
                \vdots \\ r_{i+1} \\ r_i \\ \vdots
            \end{pmatrix}
    \]
    We have:
    \begin{align*}
        LHS &= D \begin{pmatrix}
            \vdots \\ r_i \\ r_{i+1} \\ \vdots
        \end{pmatrix} + 0
        = D \begin{pmatrix}
            \vdots \\ r_i \\ r_{i+1} \\ \vdots
        \end{pmatrix} + D \begin{pmatrix}
            \vdots \\ r_{i+1} \\ r_{i+1} \\ \vdots
        \end{pmatrix} \\
        &= D \begin{pmatrix}
            \vdots \\ r_i + r_{i+1} \\ r_{i+1} \\ \vdots
        \end{pmatrix}
    \end{align*}
    Similarly, \[
    RHS = -D \begin{pmatrix}
        \vdots \\ r_i + r_{i+1} \\ r_i \\ \vdots
    \end{pmatrix}
    \]

    Thus, \[
    LHS - RHS = D \begin{pmatrix}
        \vdots \\ r_i + r_{i+1} \\ r_{i+1} \\ \vdots
    \end{pmatrix} + D\begin{pmatrix}
        \vdots \\ r_i + r_{i+1} \\ r_i \\ \vdots
    \end{pmatrix} = D\begin{pmatrix}
        \vdots \\ r_i + r_{i+1} \\ r_i + r_{i+1} \\ \vdots
    \end{pmatrix} = 0
    \]

    \pfbwd The proof backward is similar, only with the requirement that \(2 \neq 0\) in \(\bbk\).
\end{proof}

\begin{proposition}
    \(\forall n, \exists! \:\text{such a function}\: D\).
\end{proposition}

\begin{proof} {Proposition}
    If \(n=1, D: \bbk^1 \to \bbk\), since \(D\) must be multilinear (in this case, simply linear):
    \[
    D (\alpha) = D(\alpha \cdot 1) = \alpha \cdot D(1) = \alpha
    \]
    It is trivial that this \(D\) satisfies all conditions (2nd condition is satisfied as there are no 2 rows to swap) and is indeed unique.

    If \(n=2\):
    \begin{align*}
        D\begin{pmatrix}
        a & b \\
        c & d
        \end{pmatrix} &= D\begin{pmatrix}
        a & 0 \\
        c & d
        \end{pmatrix} + \begin{pmatrix}
        0 & b \\
        c & d
        \end{pmatrix} \\
        &= a D\begin{pmatrix}
        1 & 0 \\
        c & d
        \end{pmatrix} + bD\begin{pmatrix}
        0 & 1 \\
        c & d
        \end{pmatrix}\\
        &= a\left[D\begin{pmatrix}
        1 & 0 \\
        c & d
        \end{pmatrix} - cD\begin{pmatrix}
        1 & 0 \\
        1 & 0
        \end{pmatrix}\right]\\
        &+ b\left[D\begin{pmatrix}
            0 & 1 \\
            c & d
            \end{pmatrix} - dD\begin{pmatrix}
            0 & 1\\
            0 & 1\\
            \end{pmatrix}\right] \\
            &= aD\begin{pmatrix}
            1 & 0 \\
            0 & d
            \end{pmatrix} + bD\begin{pmatrix}
            0 & 1 \\
            c & 0
            \end{pmatrix} \\
            &= adD\begin{pmatrix}
            1 & 0 \\
            0 & 1
            \end{pmatrix} + bcD\begin{pmatrix}
            0 & 1 \\
            1 & 0
            \end{pmatrix} \\
            &= adD(I_2) - bcD(I_2) \\
            &= ad-bc
    \end{align*}
\end{proof}
