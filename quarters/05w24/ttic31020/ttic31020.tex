\documentclass[a4paper, 10pt]{article}
\input{../preamble.tex}
\title{TITLE}
\begin{document}
    \createintro
    % start lectures
    % end lectures
    \setcounter{section}{2}
    \section{A Formal Learning Model}
    \begin{definition}  [PAC Learnable]
        A hypothesis class $\calh$ is PAC learnable if there exists $m_{\calh}: (0, 1)^2 \to \bbn$ and a learning algorithm such that: for every $\epsilon, \delta \in (0, 1)$, distribution $\cald$ over $\calx$, labeling function $f: \calx \to \{0, 1\}$, if the realizable assumption holds with respect to $\calh, \cald, f$, then when running the algorithm on $m \geq m_{\calh} (\epsilon, \delta)$ i.i.d examples (generated by $\cald$ and labeled by $f$), the algorithm returns a hypothesis $h$ such that, with probability of $1- \delta$ (over the choice of the $m$ training samples), 
        \begin{equation*}
            L_{(D, f)}(h) \leq \epsilon
        \end{equation*}
    \end{definition}

    \begin{corollary}
    Every finite hypothesis class is PAC learnable with sample complexity \begin{equation*}
    m_{\calh}(\epsilon, \delta) \leq \ceil*{\dfrac{\log(\abs{\calh}/\delta)}{\epsilon}}
    \end{equation*}
    \end{corollary}

    \begin{definition} [Agnostic PAC Learnable]
    A hypothesis class $\calh$ is agnostic PAC learnable if there exists $m_{\calh}: (0, 1)^2 \to \bbn$ and a learning algorithm such that: for every $\epsilon, \delta \in (0, 1)$, distribution $\cald$ over $\calx \times \caly$, when running the algorithm on $m \geq m_{\calh} (\epsilon, \delta)$ i.i.d examples (generated by $\cald$), the algorithm returns a hypothesis $h$ such that, with probability $1- \delta$ (over the choice of the $m$ training samples), \begin{equation*}
    L_D(h) \leq \min_{h' \in \calh} L_D (h') + \epsilon
    \end{equation*}
    \end{definition}

    \begin{remark}
    If $\calh$ is agnostic PAC learnable then it is PAC learnable too. Because in the PAC setting, plus the realizability assumption holds, then \begin{equation*}
        \min_{h' \in \calh} L_D (h') = 0
    \end{equation*}
    and we have PAC learnability immediately.
    \end{remark}

    \begin{definition} [Generalized Loss]
    For some loss function $l: \calh \times Z \to \bbr_+$, and predictor $h \in \calh$,
    \begin{equation*}
    L_D(h) \coloneqq \bbe_{z \sim \cald}[l(h, z)]
    \end{equation*}
    \end{definition}
    \begin{definition} [Agnostic PAC Learnable for generalized loss]
        A hypothesis class $\calh$ is agnostic PAC learnable with respect to a set $Z$, loss function $l: \calh \times Z \to \bbr_{+}$ if there exists $m_{\calh}: (0, 1)^2 \to \bbn$ and a learning algorithm such that: for every $\epsilon, \delta \in (0, 1)$, distribution $\cald$ over $Z$, when running the algorithm on $m \geq m_{\calh} (\epsilon, \delta)$ i.i.d examples (generated by $\cald$), the algorithm returns a hypothesis $h$ such that, with probability $1- \delta$ (over the choice of the $m$ training samples), \begin{equation*}
        L_D(h) \leq \min_{h' \in \calh} L_D (h') + \epsilon
        \end{equation*}
        \end{definition}


   \section{Learning via Uniform Convergence}
    
    \begin{definition} [$\epsilon$-representative sample]
    A training set $S$ is called $\epsilon$-representative (wrt $Z, \calh, l, \cald$) if \begin{equation*}
    \forall h \in \calh, \abs{L_S(h) - L_\cald(h)} \geq \epsilon
    \end{equation*} 

    The name says it all: the training set is representative of the distribution if the training loss is close to the true loss, regardless of which predictor in the hypothesis class.
    \end{definition}

    \begin{lemma}
    If $S$ is $\epsilon/2$-representative then $ERM_{\calh}(S)$ satisfies \begin{equation*}
    L_D(h_S) \leq \min_{h \in \calh} L_D(h) + \epsilon
    \end{equation*}
    \end{lemma}

    \begin{definition} [Uniform convergence]
    A hypothesis class $\calh$ has the uniform convergence property (wrt $Z, l$) if there exists $m_{\calh}^{UC}: (0, 1)^2 \to \bbn$ such that: for every $\epsilon ,\delta \in (0, 1)$, distribution $\cald$ over $Z$, if $S$ is a sample of $m \geq m_{\calh}^{UC}(\epsilon, \delta)$ examples drawn i.i.d from $\cald$, then, with probability of at least $1-\delta$, $S$ is $\epsilon$-representative.

    The uniform convergence property of a hypothesis class essentially means that it is nice enough for the training sets $S$ to be representative of $\cald$.
    \end{definition}

    \begin{corollary}
        If $\calh$ has uniform convergence with $m_{\calh}^{UC}$ then it is agnostic PAC learnable with sample complexity $m_{\calh} (\epsilon, \delta) \leq m_{\calh}^{UC} (\epsilon/2, \delta)$. And $ERM_{\calh}$ paradigm is a successful agnostic PAC learner for $\calh$.
    \end{corollary}

    \begin{corollary}
    If $\calh$ is finite, $l : \calh \times Z \to [0, 1]$ (to use Hoeffding's). Then $\calh$ has uniform convergence property, that is, \begin{equation*}
    m_{\calh}^{UC} (\epsilon, \delta) \leq \ceil*{{\dfrac{\log(2 \abs{\calh}/\delta)}{2\epsilon^2}}}
    \end{equation*}

    The difference in the bound in this case and the PAC learnable case is that we're approximating around some $L_S$ that is not necessarily 0.
    \end{corollary}
    
    Since uniform convergence can be achieved, finite $\calh$ is also agnostic PAC learnable
    \begin{corollary}
    Using ERM, finite $\calh$ is agnostic PAC learnable with sample complexity:
    \begin{equation*}
    m_{\calh}(\epsilon, \delta) \leq m_{\calh}^{UC}(\epsilon/2, \delta) \leq \ceil*{2 \dfrac{\log(2\abs{\calh}/\delta)}{2(\epsilon/2)^2}} = \ceil*{\dfrac{2\log(2\abs{\calh}/\delta)}{\epsilon^2}}
    \end{equation*}
    \end{corollary}
    
    \section{The Bias-Complexity Tradeoff}

    \begin{definition} [Error Decomposition]
    \begin{equation*}
    L_D(h_S) = \epsilon_{app} + \epsilon_{est}
    \end{equation*}
    where $\epsilon_{app} = \min_{h \in \calh}L_D(h), \epsilon_{est} = L_\cald(h_S) - \epsilon_{app}$.
    \end{definition}

    Note that $\epsilon_{app}$ does not depend on the training set, nor the hypothesis. It solely depends on the hypothesis class $\calh$. It decreases when enlarging the hypothesis class. This term measures how much risk we've restricted ourselves to, i.e., how much inductive bias we have.

    Meanwhile, for finite $\calh$, as we've shown $\epsilon_{est}$ increases (logarithmically) with $\abs{\calh}$ and decreases with $m$.
   \section{The VC-Dimension}
    \begin{definition} [Shattering]
    $\calh$ shatters a finite set $C \subset \calx$ if the restriction of $\calh$ to $C$ is the set of all functions from $C$ to $\{0, 1\}$. That is, $\abs{\calh_C} = 2^{\abs{C}}$.
    \end{definition}

    \begin{definition} [VC-dimension]
    The VC-dimension of a hypothesis class $\calh$, denoted $VCDim(\calh)$, is the maximal size of a set $C \subset \calx$ that can be shattered by $\calh$. It is $\infty$ when $\calh$ can shatter sets of arbitrarily large size.
    \end{definition}

    \begin{theorem}
    Let $\calh$ have $VCDim(\calh) = \infty$. Then $\calh$ is not PAC learnable.
    \end{theorem}

    The converse is also true.
    \begin{theorem} [Fundamental Theorem of Statistical Learning]
        Let $\calh$ be a hypothesis class from $\calx$ to $\caly = \{0, 1\}$, let loss function be 0-1 loss. Then, TFAE:
        \begin{enumerate}
        \item $\calh$ has uniform convergence property.
        \item Any ERM rule is a successful agnostic PAC learner for $\calh$.
        \item $\calh$ is agnostic PAC learnable.
        \item $\calh$ is PAC learnable.
        \item Any ERM rule is a successful PAC learner for $\calh$.
        \item $\calh$ has a finite VC-dimension.
        \end{enumerate}

    Not only does the VC-dimension characterize PAC learnability; it even determines the sample complexity.
    \end{theorem}

    \begin{remark}
    This extends to regression with absolute/squared loss. However, theorem does not hold for all learning tasks. In particular, learnability is sometimes possible, even though uniform convergence does not hold. Someetimes, ERM rule fails but learnability is possible with other learning rules.
    \end{remark}

    \section{Nonuniform Learnability}
    \begin{definition} [Nonuniformly Learnable]
    $\calh$ is nonuniformly learnable if there exists a learning algorithm, $A$, and $m_{\calh}^{NUL}: (0, 1)^2 \times \calh \to \bbn$ such that, for every $\epsilon, \delta \in (0, 1)$, $h \in \calh$, if $m \geq m_{\calh}^{NUL}(\epsilon, \delta, h)$ then for every distribution $\cald$, with probability of at least $1 - \delta$ over the choice of $S \sim \cald^m$, we have \begin{equation*}
    L_\cald (A(S)) \leq L_\cald (h) + \epsilon
    \end{equation*}
    i.e., that $A(S)$ is $(\epsilon, \delta)$-competitive with every other hypothesis in the class.
    \end{definition}

    \begin{remark}
    Recall that agnostic PAC learnability also requires \begin{equation*}
    L_\cald (A(S)) \leq \min_{h' \in \calh}(L_\cald(h')) + \epsilon \leq L_\cald(h') + \epsilon \forall h' \in \calh
    \end{equation*}
    which essentially requires $A(S)$ to be $(\epsilon, \delta)$-competitive with every hypothesis in $\calh$ too. But the difference is that in agnostic PAC, $m_{\calh}$   is only allowed to depend on $(\epsilon, \delta)$ while for nonuniform learnability, $m_{\calh}^{NUL}$ is allowed to depend on $(\epsilon, \delta, h)$; thus the ``non-uniform''ness.

    It is thus an easy consequence that agnostic PAC learnable $\implies$ nonuniformly learnable.
    \end{remark}

    \begin{theorem}
    $\calh$ of binary classifiers is nonuniformly learnable iff it is a countable union of agnostic PAC learnable hypothesis classes.
    \end{theorem}

    \begin{remark}
    Look up nonuniform learnable again.
    \end{remark}

    \begin{theorem}
        Let $\calh = \bigcup_{n \in \bbn} \calh_n$ where $\calh_n$ has uniform convergence with $m_{\calh_n}^{UC}$. Let $w: \bbn \to [0, 1], w(n) = \frac{6}{\pi^2n^2}$. Then $\calh$ is uniformly learnable using SRM rule with rate \begin{equation*}
        m_{\calh}^{NUL} \leq m^{UC}_{\calh_{n(h)}}\left(\epsilon/2, \dfrac{6\delta}{(\pi n(h))^2}\right)
        \end{equation*}
    \end{theorem}

    SRM Rule equivalently by applying Hoeffding's inequality, with $\calh = \bigcup_{n \in \bbn}\{h_n\}$ for countable hypothesis class.
    \begin{equation*}
    SRM(S) = \argmin_{h \in \calh} \left[L_S(h) + \sqrt{\dfrac{-\log(w(h)) + \log(2/\delta)}{2m}}\right]
    \end{equation*}
    so we assign more weight to hypotheses that are more likely to be correct.

    \hrule
    So far in the book, we have studied the statistical perspective of learning, namely how many samples are needed for learning (hence, \textit{sample complexity}). Now we turn to \textit{computational complexity}.
\end{document}